{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing text data\n",
    "\n",
    "With clean data we can begin to ask what is the best way to extract features from the data. There are many more approaches for text analytics and natural language processing (NLP). We only mention a few below. Note that the collection of unique words in the data is called a **vocabulary**. To avoid having a vocabulary that's too large, we can trim it by keeping the most frequent $N$ words, making $N$ the size of the vocabulary. A **document** usually refers to a single data point with raw text, such as a tweet, a review, an invoice, etc. So our documents are made up of \"words\" that come from the corpus (ignoring any words that are not in the vocabulary). The question now is how do we represent such a data numerically? Here are two approaches:\n",
    "\n",
    "- The **bag of words model (BoW)** is a simple and surprisingly effective model for analysis of text data. The BoW model creates a **sparse vector representation** of each word in the corpus based on the frequency of the words in the document. The order of the words is not considered, nor is the similarity between different words. Despite serious shortcomings, the model can work well in many cases.\n",
    "- We can usually do much better by using **word embeddings**, which are **dense vector respresentations** for each word in the corpus. Word embeddings are learned by examining the word's **context** (other words around it). Word embeddings are very common in **deep learning** applications of NLP, although the embeddings themselves are learned using a shallow network. If we learn word embeddings from a very large data set once, we can save and re-use these word embeddings to create features for other data sets. In fact, **pre-trained word embeddings** are trained by large companies like Google and made available for use by others. So we can load these embeddings and numerically represent a document using the average of the embeddings of the words in it. Because word embeddings are vectors, such an average would also be a vector that is a dense representation of the document.\n",
    "\n",
    "As you can see, BoW models seem too simplistic and word embeddings seem a bit too sophisticated (I mean in the context of DATASCI 510 course). So here's another approach that is sort of between the two in terms of difficulty. It is called **TF-IDF** and it is a clever way to featurize words in documents. Just like a BoW model, we begin by \"tokenizing\" the data. In BoW we then create a one-hot encoded feature for each token (or word). But in TF-IDF we first extract the relative word frequencies per document (called **term frequencies** or TF), we then multiply the term frequencies by a multiplier we call IDF. This has the effect of dampening the values for terms that appear frequently across documents, giving them less influence when we move on to the machine learning phase. Note that we used the words \"token\", \"word\" and \"term\" almost interchangeably. Sorry for confusing you! Data scientists don't always agree on terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "0  POLIT  Global Voices Online Â» Alex Castro: A liberal...\n",
       "1  POLIT  Do the Conservatives Have a Death Wish? http:/...\n",
       "2    NOT  @MMFlint I've seen all of your movies and Capi...\n",
       "3  POLIT  RT @AllianceAlert: * House Dems ask for civili...\n",
       "4  POLIT  RT @AdamSmithInst Quote of the week: My politi..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1) use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "# url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "# url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "url_kt = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "url_gt = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "keyword_tweets=pd.read_csv(url_kt,sep='\\t',header=None)\n",
    "general_tweets=pd.read_csv(url_gt,sep='\\t',header=None)\n",
    "\n",
    "keyword_tweets.head()\n",
    "# general_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@themoderngal ditto for me. i am having remors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@ceebrito wats goodie my dominican brotha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>NOT</td>\n",
       "      <td>yea my fone iz a DUBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@camerongarcia oh yes! My mom wanted to buy my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @RedState: Voter Fraud Video Watch - NJ. ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                              Tweet\n",
       "0        POLIT  Global Voices Online Â» Alex Castro: A liberal...\n",
       "1        POLIT  Do the Conservatives Have a Death Wish? http:/...\n",
       "2          NOT  @MMFlint I've seen all of your movies and Capi...\n",
       "3        POLIT  RT @AllianceAlert: * House Dems ask for civili...\n",
       "4        POLIT  RT @AdamSmithInst Quote of the week: My politi...\n",
       "...        ...                                                ...\n",
       "3999       NOT  @themoderngal ditto for me. i am having remors...\n",
       "4000       NOT          @ceebrito wats goodie my dominican brotha\n",
       "4001       NOT                              yea my fone iz a DUBB\n",
       "4002       NOT  @camerongarcia oh yes! My mom wanted to buy my...\n",
       "4003     POLIT  RT @RedState: Voter Fraud Video Watch - NJ. ht...\n",
       "\n",
       "[4004 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "LabeledTweets = pd.concat([keyword_tweets, general_tweets], ignore_index=True)\n",
    "LabeledTweets=LabeledTweets.rename(columns={0:'Sentiment' ,1:'Tweet'})\n",
    "# help(pd.concat)  # Shows source code if available, or detailed info\n",
    "LabeledTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4004, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3)'POLIT': 1, 'NOT': 0;\n",
    "map_rep={'POLIT': 1, 'NOT': 0}\n",
    "LabeledTweets.replace(map_rep,inplace=True)\n",
    "LabeledTweets.head()\n",
    "LabeledTweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)clean the tweets\n",
    "\n",
    "# remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "# remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "# replace (not remove) all punctuation marks with a space (\" \")\n",
    "# replace all numbers with a space\n",
    "# replace all non ascii characters with a space\n",
    "# convert all characters to lowercase\n",
    "# strip extra whitespaces\n",
    "# lemmatize tokens\n",
    "# No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "\n",
    "# remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "LabeledTweets_no_at = LabeledTweets.loc[~LabeledTweets['Tweet'].str.contains('@', na=False)]\n",
    "LabeledTweets_no_at\n",
    "# remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "LabeledTweets_notHttp=LabeledTweets_no_at.loc[~LabeledTweets_no_at['Tweet'].str.contains('http', na=False)]\n",
    "LabeledTweets_notHttp\n",
    "\n",
    "\n",
    "# replace (not remove) all punctuation marks with a space (\" \")\n",
    "# print(string.punctuation)\n",
    "LabeledTweets_notHttp.loc[:,'Tweet']=LabeledTweets_notHttp['Tweet'].str.replace(f\"[{string.punctuation}]\",\" \",regex=True)\n",
    "# #replace all numbers with a space\n",
    "LabeledTweets_notHttp.loc[:,'Tweet']=LabeledTweets_notHttp['Tweet'].str.replace(\"\\d+\",\" \",regex=True)\n",
    "# #replace all non ascii characters with a space\n",
    "LabeledTweets_notHttp.loc[:,'Tweet'] = LabeledTweets_notHttp['Tweet'].str.replace(r'[^\\x00-\\x7F]+', ' ', regex=True)\n",
    "# convert all characters to lowercase\n",
    "LabeledTweets_notHttp.loc[:,'Tweet'] = LabeledTweets_notHttp['Tweet'].str.lower()\n",
    "\n",
    "# strip extra whitespaces\n",
    "LabeledTweets_notHttp.loc[:,'Tweet'] = LabeledTweets_notHttp['Tweet'].str.strip()\n",
    "\n",
    "# lemmatize\n",
    "# Initialize the lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word in the 'Tweet' column\n",
    "LabeledTweets_notHttp.loc[:,'Tweet'] = LabeledTweets_notHttp['Tweet'].apply(\n",
    "    lambda text: \" \".join([lmtzr.lemmatize(word) for word in text.split()])\n",
    ")\n",
    "\n",
    "print(LabeledTweets_notHttp['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;\n",
    "clean_texts = LabeledTweets_notHttp['Tweet']\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 50, stop_words = 'english')\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 50)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "print(tfidf_df.shape)\n",
    "tfidf_df.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
